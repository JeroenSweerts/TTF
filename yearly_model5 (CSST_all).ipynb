{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yearly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd, nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>% FULL</th>\n",
       "      <th>Brent (eur/bbl)</th>\n",
       "      <th>JKM (Eur/mmbtu)</th>\n",
       "      <th>Coal (eur/t)</th>\n",
       "      <th>CO2 (eur/t)</th>\n",
       "      <th>Endex (eur/Mwh)</th>\n",
       "      <th>AVG_TEMP</th>\n",
       "      <th>TTF (eur/Mwh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>4/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.383127</td>\n",
       "      <td>-0.646412</td>\n",
       "      <td>-0.145412</td>\n",
       "      <td>-0.874372</td>\n",
       "      <td>-0.650510</td>\n",
       "      <td>-0.907780</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>14.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>5/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.367589</td>\n",
       "      <td>-0.665730</td>\n",
       "      <td>-0.140441</td>\n",
       "      <td>-0.889447</td>\n",
       "      <td>-0.654762</td>\n",
       "      <td>-0.905267</td>\n",
       "      <td>-0.974684</td>\n",
       "      <td>15.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>6/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.353414</td>\n",
       "      <td>-0.751741</td>\n",
       "      <td>-0.151809</td>\n",
       "      <td>-0.862647</td>\n",
       "      <td>-0.676020</td>\n",
       "      <td>-0.892701</td>\n",
       "      <td>-0.924051</td>\n",
       "      <td>15.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>7/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.338422</td>\n",
       "      <td>-0.787722</td>\n",
       "      <td>-0.187518</td>\n",
       "      <td>-0.842546</td>\n",
       "      <td>-0.693027</td>\n",
       "      <td>-0.887238</td>\n",
       "      <td>-0.924051</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>8/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.325065</td>\n",
       "      <td>-0.794534</td>\n",
       "      <td>-0.186657</td>\n",
       "      <td>-0.889447</td>\n",
       "      <td>-0.704082</td>\n",
       "      <td>-0.899366</td>\n",
       "      <td>-0.772152</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>11/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.293989</td>\n",
       "      <td>-0.860836</td>\n",
       "      <td>-0.174514</td>\n",
       "      <td>-0.902848</td>\n",
       "      <td>-0.728741</td>\n",
       "      <td>-0.896525</td>\n",
       "      <td>-0.493671</td>\n",
       "      <td>15.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>12/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.282813</td>\n",
       "      <td>-0.885936</td>\n",
       "      <td>-0.179240</td>\n",
       "      <td>-0.926298</td>\n",
       "      <td>-0.730442</td>\n",
       "      <td>-0.907124</td>\n",
       "      <td>-0.392405</td>\n",
       "      <td>14.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>13/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.270001</td>\n",
       "      <td>-0.908342</td>\n",
       "      <td>-0.186736</td>\n",
       "      <td>-0.943049</td>\n",
       "      <td>-0.718537</td>\n",
       "      <td>-0.900896</td>\n",
       "      <td>-0.367089</td>\n",
       "      <td>14.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>14/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.254191</td>\n",
       "      <td>-0.880680</td>\n",
       "      <td>-0.189371</td>\n",
       "      <td>-0.948074</td>\n",
       "      <td>-0.732143</td>\n",
       "      <td>-0.899913</td>\n",
       "      <td>-0.392405</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>19/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.160420</td>\n",
       "      <td>-0.968968</td>\n",
       "      <td>-0.541956</td>\n",
       "      <td>-0.966499</td>\n",
       "      <td>-0.755102</td>\n",
       "      <td>-0.882976</td>\n",
       "      <td>-0.721519</td>\n",
       "      <td>14.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>20/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.138067</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.589939</td>\n",
       "      <td>-0.934673</td>\n",
       "      <td>-0.797619</td>\n",
       "      <td>-0.883960</td>\n",
       "      <td>-0.848101</td>\n",
       "      <td>13.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>21/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.115987</td>\n",
       "      <td>-0.947562</td>\n",
       "      <td>-0.633740</td>\n",
       "      <td>-0.932998</td>\n",
       "      <td>-0.809524</td>\n",
       "      <td>-0.895323</td>\n",
       "      <td>-0.620253</td>\n",
       "      <td>13.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>22/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.095543</td>\n",
       "      <td>-0.830239</td>\n",
       "      <td>-0.628989</td>\n",
       "      <td>-0.906198</td>\n",
       "      <td>-0.796769</td>\n",
       "      <td>-0.902644</td>\n",
       "      <td>-0.746835</td>\n",
       "      <td>14.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>25/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.056017</td>\n",
       "      <td>-0.898582</td>\n",
       "      <td>-0.636031</td>\n",
       "      <td>-0.932998</td>\n",
       "      <td>-0.835034</td>\n",
       "      <td>-0.910293</td>\n",
       "      <td>-0.316456</td>\n",
       "      <td>13.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>26/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.044841</td>\n",
       "      <td>-0.852637</td>\n",
       "      <td>-0.638934</td>\n",
       "      <td>-0.926298</td>\n",
       "      <td>-0.818027</td>\n",
       "      <td>-0.910948</td>\n",
       "      <td>-0.164557</td>\n",
       "      <td>13.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>27/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.033938</td>\n",
       "      <td>-0.806878</td>\n",
       "      <td>-0.659960</td>\n",
       "      <td>-0.911223</td>\n",
       "      <td>-0.832483</td>\n",
       "      <td>-0.909637</td>\n",
       "      <td>-0.063291</td>\n",
       "      <td>13.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>28/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.023034</td>\n",
       "      <td>-0.782901</td>\n",
       "      <td>-0.665892</td>\n",
       "      <td>-0.912898</td>\n",
       "      <td>-0.818878</td>\n",
       "      <td>-0.909200</td>\n",
       "      <td>-0.012658</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>29/01/2016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.013221</td>\n",
       "      <td>-0.739376</td>\n",
       "      <td>-0.652540</td>\n",
       "      <td>-0.926298</td>\n",
       "      <td>-0.820578</td>\n",
       "      <td>-0.910184</td>\n",
       "      <td>-0.139241</td>\n",
       "      <td>13.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>1/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.010495</td>\n",
       "      <td>-0.764079</td>\n",
       "      <td>-0.645356</td>\n",
       "      <td>-0.963149</td>\n",
       "      <td>-0.851190</td>\n",
       "      <td>-0.954545</td>\n",
       "      <td>-0.291139</td>\n",
       "      <td>13.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>2/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.019490</td>\n",
       "      <td>-0.823726</td>\n",
       "      <td>-0.614478</td>\n",
       "      <td>-0.959799</td>\n",
       "      <td>-0.840136</td>\n",
       "      <td>-0.965144</td>\n",
       "      <td>-0.113924</td>\n",
       "      <td>13.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>3/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.024397</td>\n",
       "      <td>-0.759812</td>\n",
       "      <td>-0.618472</td>\n",
       "      <td>-0.919598</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.971263</td>\n",
       "      <td>-0.063291</td>\n",
       "      <td>13.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>4/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.036118</td>\n",
       "      <td>-0.792386</td>\n",
       "      <td>-0.611479</td>\n",
       "      <td>-0.943049</td>\n",
       "      <td>-0.857993</td>\n",
       "      <td>-0.967111</td>\n",
       "      <td>-0.240506</td>\n",
       "      <td>12.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>5/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.046204</td>\n",
       "      <td>-0.801289</td>\n",
       "      <td>-0.604941</td>\n",
       "      <td>-0.974874</td>\n",
       "      <td>-0.863095</td>\n",
       "      <td>-0.973558</td>\n",
       "      <td>-0.367089</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>8/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.068557</td>\n",
       "      <td>-0.848091</td>\n",
       "      <td>-0.610046</td>\n",
       "      <td>-0.994975</td>\n",
       "      <td>-0.889456</td>\n",
       "      <td>-0.976071</td>\n",
       "      <td>-0.164557</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>9/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.078097</td>\n",
       "      <td>-0.949608</td>\n",
       "      <td>-0.622462</td>\n",
       "      <td>-0.993300</td>\n",
       "      <td>-0.913265</td>\n",
       "      <td>-0.971919</td>\n",
       "      <td>-0.088608</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>10/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.088728</td>\n",
       "      <td>-0.930838</td>\n",
       "      <td>-0.622205</td>\n",
       "      <td>-0.994975</td>\n",
       "      <td>-0.920068</td>\n",
       "      <td>-0.976836</td>\n",
       "      <td>-0.088608</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>11/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.100450</td>\n",
       "      <td>-0.961740</td>\n",
       "      <td>-0.626298</td>\n",
       "      <td>-0.998325</td>\n",
       "      <td>-0.931973</td>\n",
       "      <td>-0.977928</td>\n",
       "      <td>-0.215190</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>12/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.111081</td>\n",
       "      <td>-0.837523</td>\n",
       "      <td>-0.618218</td>\n",
       "      <td>-0.988275</td>\n",
       "      <td>-0.904762</td>\n",
       "      <td>-0.975524</td>\n",
       "      <td>-0.316456</td>\n",
       "      <td>12.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>16/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.152787</td>\n",
       "      <td>-0.867690</td>\n",
       "      <td>-0.788429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.937075</td>\n",
       "      <td>-0.989292</td>\n",
       "      <td>-0.341772</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>17/02/2016</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.167507</td>\n",
       "      <td>-0.782131</td>\n",
       "      <td>-0.786817</td>\n",
       "      <td>-0.994975</td>\n",
       "      <td>-0.903061</td>\n",
       "      <td>-0.992898</td>\n",
       "      <td>-0.518987</td>\n",
       "      <td>12.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>26/04/2018</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.829085</td>\n",
       "      <td>0.458407</td>\n",
       "      <td>-0.086602</td>\n",
       "      <td>0.391960</td>\n",
       "      <td>-0.191327</td>\n",
       "      <td>-0.786167</td>\n",
       "      <td>-0.037975</td>\n",
       "      <td>20.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>27/04/2018</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.820363</td>\n",
       "      <td>0.449536</td>\n",
       "      <td>-0.077239</td>\n",
       "      <td>0.388610</td>\n",
       "      <td>-0.185374</td>\n",
       "      <td>-0.790319</td>\n",
       "      <td>-0.037975</td>\n",
       "      <td>20.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>30/04/2018</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.789560</td>\n",
       "      <td>0.477718</td>\n",
       "      <td>-0.062532</td>\n",
       "      <td>0.413735</td>\n",
       "      <td>-0.182823</td>\n",
       "      <td>-0.728912</td>\n",
       "      <td>-0.037975</td>\n",
       "      <td>20.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.777021</td>\n",
       "      <td>0.426696</td>\n",
       "      <td>-0.048470</td>\n",
       "      <td>0.480737</td>\n",
       "      <td>-0.199830</td>\n",
       "      <td>-0.728038</td>\n",
       "      <td>-0.113924</td>\n",
       "      <td>20.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>2/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.771296</td>\n",
       "      <td>0.443308</td>\n",
       "      <td>-0.041194</td>\n",
       "      <td>0.463987</td>\n",
       "      <td>-0.227041</td>\n",
       "      <td>-0.732955</td>\n",
       "      <td>-0.088608</td>\n",
       "      <td>20.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>3/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.762846</td>\n",
       "      <td>0.444414</td>\n",
       "      <td>-0.057597</td>\n",
       "      <td>0.463987</td>\n",
       "      <td>-0.235544</td>\n",
       "      <td>-0.735686</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>19.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>4/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.757667</td>\n",
       "      <td>0.492582</td>\n",
       "      <td>-0.052716</td>\n",
       "      <td>0.460637</td>\n",
       "      <td>-0.230442</td>\n",
       "      <td>-0.730332</td>\n",
       "      <td>-0.113924</td>\n",
       "      <td>20.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>7/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.724138</td>\n",
       "      <td>0.544619</td>\n",
       "      <td>-0.046284</td>\n",
       "      <td>0.460637</td>\n",
       "      <td>-0.179422</td>\n",
       "      <td>-0.724432</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>20.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>8/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.712689</td>\n",
       "      <td>0.512339</td>\n",
       "      <td>-0.036388</td>\n",
       "      <td>0.497487</td>\n",
       "      <td>-0.181122</td>\n",
       "      <td>-0.713505</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>20.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>9/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.701513</td>\n",
       "      <td>0.595240</td>\n",
       "      <td>-0.031764</td>\n",
       "      <td>0.541039</td>\n",
       "      <td>-0.147109</td>\n",
       "      <td>-0.704764</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>20.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>10/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.689791</td>\n",
       "      <td>0.590146</td>\n",
       "      <td>-0.042544</td>\n",
       "      <td>0.541039</td>\n",
       "      <td>-0.095238</td>\n",
       "      <td>-0.703671</td>\n",
       "      <td>0.316456</td>\n",
       "      <td>20.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>11/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.677252</td>\n",
       "      <td>0.571956</td>\n",
       "      <td>-0.042379</td>\n",
       "      <td>0.624791</td>\n",
       "      <td>-0.092687</td>\n",
       "      <td>-0.697443</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>21.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>14/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.639907</td>\n",
       "      <td>0.613211</td>\n",
       "      <td>-0.039489</td>\n",
       "      <td>0.643216</td>\n",
       "      <td>-0.092687</td>\n",
       "      <td>-0.692635</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>21.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>15/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.629276</td>\n",
       "      <td>0.639696</td>\n",
       "      <td>-0.024391</td>\n",
       "      <td>0.643216</td>\n",
       "      <td>-0.119048</td>\n",
       "      <td>-0.678977</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>21.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>16/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.618918</td>\n",
       "      <td>0.675522</td>\n",
       "      <td>0.142940</td>\n",
       "      <td>0.673367</td>\n",
       "      <td>-0.045918</td>\n",
       "      <td>-0.684113</td>\n",
       "      <td>0.316456</td>\n",
       "      <td>21.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>17/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.608559</td>\n",
       "      <td>0.679421</td>\n",
       "      <td>0.216423</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>-0.035714</td>\n",
       "      <td>-0.672640</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>22.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>18/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.598746</td>\n",
       "      <td>0.656507</td>\n",
       "      <td>0.279271</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>-0.039966</td>\n",
       "      <td>-0.675809</td>\n",
       "      <td>-0.012658</td>\n",
       "      <td>22.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>21/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.560311</td>\n",
       "      <td>0.677374</td>\n",
       "      <td>0.314940</td>\n",
       "      <td>0.681742</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.675262</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>22.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>22/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.548862</td>\n",
       "      <td>0.692121</td>\n",
       "      <td>0.379287</td>\n",
       "      <td>0.698492</td>\n",
       "      <td>0.019558</td>\n",
       "      <td>-0.660948</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>22.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>23/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.540684</td>\n",
       "      <td>0.719155</td>\n",
       "      <td>0.352296</td>\n",
       "      <td>0.644891</td>\n",
       "      <td>0.024660</td>\n",
       "      <td>-0.659965</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>22.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>24/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.528963</td>\n",
       "      <td>0.678994</td>\n",
       "      <td>0.341059</td>\n",
       "      <td>0.651591</td>\n",
       "      <td>0.048469</td>\n",
       "      <td>-0.658108</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>22.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>25/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.517241</td>\n",
       "      <td>0.613697</td>\n",
       "      <td>0.352732</td>\n",
       "      <td>0.631491</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.665210</td>\n",
       "      <td>0.392405</td>\n",
       "      <td>21.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>29/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.469265</td>\n",
       "      <td>0.602682</td>\n",
       "      <td>0.385470</td>\n",
       "      <td>0.690117</td>\n",
       "      <td>0.049320</td>\n",
       "      <td>-0.632430</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>22.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>30/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.459180</td>\n",
       "      <td>0.647863</td>\n",
       "      <td>0.359567</td>\n",
       "      <td>0.690117</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>-0.613418</td>\n",
       "      <td>0.367089</td>\n",
       "      <td>22.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>31/05/2018</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.449911</td>\n",
       "      <td>0.644319</td>\n",
       "      <td>0.353539</td>\n",
       "      <td>0.778894</td>\n",
       "      <td>-0.068878</td>\n",
       "      <td>-0.649585</td>\n",
       "      <td>0.392405</td>\n",
       "      <td>22.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>1/06/2018</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.441188</td>\n",
       "      <td>0.623534</td>\n",
       "      <td>0.371502</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>-0.036565</td>\n",
       "      <td>-0.669580</td>\n",
       "      <td>0.367089</td>\n",
       "      <td>21.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>4/06/2018</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.405206</td>\n",
       "      <td>0.563189</td>\n",
       "      <td>0.375259</td>\n",
       "      <td>0.785595</td>\n",
       "      <td>0.034864</td>\n",
       "      <td>-0.650240</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>22.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>5/06/2018</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.397301</td>\n",
       "      <td>0.562077</td>\n",
       "      <td>0.382963</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>-0.670673</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>21.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>6/06/2018</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.388578</td>\n",
       "      <td>0.549050</td>\n",
       "      <td>0.367422</td>\n",
       "      <td>0.731323</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>-0.679414</td>\n",
       "      <td>0.113924</td>\n",
       "      <td>21.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>7/06/2018</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.379310</td>\n",
       "      <td>0.610807</td>\n",
       "      <td>0.376630</td>\n",
       "      <td>0.757119</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.676901</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>21.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3951 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date     month    % FULL  Brent (eur/bbl)  JKM (Eur/mmbtu)  \\\n",
       "431    4/01/2016 -1.000000  0.383127        -0.646412        -0.145412   \n",
       "432    5/01/2016 -1.000000  0.367589        -0.665730        -0.140441   \n",
       "433    6/01/2016 -1.000000  0.353414        -0.751741        -0.151809   \n",
       "434    7/01/2016 -1.000000  0.338422        -0.787722        -0.187518   \n",
       "435    8/01/2016 -1.000000  0.325065        -0.794534        -0.186657   \n",
       "436   11/01/2016 -1.000000  0.293989        -0.860836        -0.174514   \n",
       "437   12/01/2016 -1.000000  0.282813        -0.885936        -0.179240   \n",
       "438   13/01/2016 -1.000000  0.270001        -0.908342        -0.186736   \n",
       "439   14/01/2016 -1.000000  0.254191        -0.880680        -0.189371   \n",
       "440   19/01/2016 -1.000000  0.160420        -0.968968        -0.541956   \n",
       "441   20/01/2016 -1.000000  0.138067        -1.000000        -0.589939   \n",
       "442   21/01/2016 -1.000000  0.115987        -0.947562        -0.633740   \n",
       "443   22/01/2016 -1.000000  0.095543        -0.830239        -0.628989   \n",
       "444   25/01/2016 -1.000000  0.056017        -0.898582        -0.636031   \n",
       "445   26/01/2016 -1.000000  0.044841        -0.852637        -0.638934   \n",
       "446   27/01/2016 -1.000000  0.033938        -0.806878        -0.659960   \n",
       "447   28/01/2016 -1.000000  0.023034        -0.782901        -0.665892   \n",
       "448   29/01/2016 -1.000000  0.013221        -0.739376        -0.652540   \n",
       "449    1/02/2016 -0.818182 -0.010495        -0.764079        -0.645356   \n",
       "450    2/02/2016 -0.818182 -0.019490        -0.823726        -0.614478   \n",
       "451    3/02/2016 -0.818182 -0.024397        -0.759812        -0.618472   \n",
       "452    4/02/2016 -0.818182 -0.036118        -0.792386        -0.611479   \n",
       "453    5/02/2016 -0.818182 -0.046204        -0.801289        -0.604941   \n",
       "454    8/02/2016 -0.818182 -0.068557        -0.848091        -0.610046   \n",
       "455    9/02/2016 -0.818182 -0.078097        -0.949608        -0.622462   \n",
       "456   10/02/2016 -0.818182 -0.088728        -0.930838        -0.622205   \n",
       "457   11/02/2016 -0.818182 -0.100450        -0.961740        -0.626298   \n",
       "458   12/02/2016 -0.818182 -0.111081        -0.837523        -0.618218   \n",
       "459   16/02/2016 -0.818182 -0.152787        -0.867690        -0.788429   \n",
       "460   17/02/2016 -0.818182 -0.167507        -0.782131        -0.786817   \n",
       "...          ...       ...       ...              ...              ...   \n",
       "1004  26/04/2018 -0.454545 -0.829085         0.458407        -0.086602   \n",
       "1005  27/04/2018 -0.454545 -0.820363         0.449536        -0.077239   \n",
       "1006  30/04/2018 -0.454545 -0.789560         0.477718        -0.062532   \n",
       "1007   1/05/2018 -0.272727 -0.777021         0.426696        -0.048470   \n",
       "1008   2/05/2018 -0.272727 -0.771296         0.443308        -0.041194   \n",
       "1009   3/05/2018 -0.272727 -0.762846         0.444414        -0.057597   \n",
       "1010   4/05/2018 -0.272727 -0.757667         0.492582        -0.052716   \n",
       "1011   7/05/2018 -0.272727 -0.724138         0.544619        -0.046284   \n",
       "1012   8/05/2018 -0.272727 -0.712689         0.512339        -0.036388   \n",
       "1013   9/05/2018 -0.272727 -0.701513         0.595240        -0.031764   \n",
       "1014  10/05/2018 -0.272727 -0.689791         0.590146        -0.042544   \n",
       "1015  11/05/2018 -0.272727 -0.677252         0.571956        -0.042379   \n",
       "1016  14/05/2018 -0.272727 -0.639907         0.613211        -0.039489   \n",
       "1017  15/05/2018 -0.272727 -0.629276         0.639696        -0.024391   \n",
       "1018  16/05/2018 -0.272727 -0.618918         0.675522         0.142940   \n",
       "1019  17/05/2018 -0.272727 -0.608559         0.679421         0.216423   \n",
       "1020  18/05/2018 -0.272727 -0.598746         0.656507         0.279271   \n",
       "1021  21/05/2018 -0.272727 -0.560311         0.677374         0.314940   \n",
       "1022  22/05/2018 -0.272727 -0.548862         0.692121         0.379287   \n",
       "1023  23/05/2018 -0.272727 -0.540684         0.719155         0.352296   \n",
       "1024  24/05/2018 -0.272727 -0.528963         0.678994         0.341059   \n",
       "1025  25/05/2018 -0.272727 -0.517241         0.613697         0.352732   \n",
       "1026  29/05/2018 -0.272727 -0.469265         0.602682         0.385470   \n",
       "1027  30/05/2018 -0.272727 -0.459180         0.647863         0.359567   \n",
       "1028  31/05/2018 -0.272727 -0.449911         0.644319         0.353539   \n",
       "1029   1/06/2018 -0.090909 -0.441188         0.623534         0.371502   \n",
       "1030   4/06/2018 -0.090909 -0.405206         0.563189         0.375259   \n",
       "1031   5/06/2018 -0.090909 -0.397301         0.562077         0.382963   \n",
       "1032   6/06/2018 -0.090909 -0.388578         0.549050         0.367422   \n",
       "1033   7/06/2018 -0.090909 -0.379310         0.610807         0.376630   \n",
       "\n",
       "      Coal (eur/t)  CO2 (eur/t)  Endex (eur/Mwh)  AVG_TEMP  TTF (eur/Mwh)  \n",
       "431      -0.874372    -0.650510        -0.907780 -1.000000          14.90  \n",
       "432      -0.889447    -0.654762        -0.905267 -0.974684          15.25  \n",
       "433      -0.862647    -0.676020        -0.892701 -0.924051          15.45  \n",
       "434      -0.842546    -0.693027        -0.887238 -0.924051          16.00  \n",
       "435      -0.889447    -0.704082        -0.899366 -0.772152          15.10  \n",
       "436      -0.902848    -0.728741        -0.896525 -0.493671          15.15  \n",
       "437      -0.926298    -0.730442        -0.907124 -0.392405          14.45  \n",
       "438      -0.943049    -0.718537        -0.900896 -0.367089          14.60  \n",
       "439      -0.948074    -0.732143        -0.899913 -0.392405          14.10  \n",
       "440      -0.966499    -0.755102        -0.882976 -0.721519          14.30  \n",
       "441      -0.934673    -0.797619        -0.883960 -0.848101          13.68  \n",
       "442      -0.932998    -0.809524        -0.895323 -0.620253          13.53  \n",
       "443      -0.906198    -0.796769        -0.902644 -0.746835          14.25  \n",
       "444      -0.932998    -0.835034        -0.910293 -0.316456          13.23  \n",
       "445      -0.926298    -0.818027        -0.910948 -0.164557          13.40  \n",
       "446      -0.911223    -0.832483        -0.909637 -0.063291          13.57  \n",
       "447      -0.912898    -0.818878        -0.909200 -0.012658          14.00  \n",
       "448      -0.926298    -0.820578        -0.910184 -0.139241          13.75  \n",
       "449      -0.963149    -0.851190        -0.954545 -0.291139          13.28  \n",
       "450      -0.959799    -0.840136        -0.965144 -0.113924          13.20  \n",
       "451      -0.919598    -0.857143        -0.971263 -0.063291          13.45  \n",
       "452      -0.943049    -0.857993        -0.967111 -0.240506          12.88  \n",
       "453      -0.974874    -0.863095        -0.973558 -0.367089          12.80  \n",
       "454      -0.994975    -0.889456        -0.976071 -0.164557          12.60  \n",
       "455      -0.993300    -0.913265        -0.971919 -0.088608          12.60  \n",
       "456      -0.994975    -0.920068        -0.976836 -0.088608          12.80  \n",
       "457      -0.998325    -0.931973        -0.977928 -0.215190          12.43  \n",
       "458      -0.988275    -0.904762        -0.975524 -0.316456          12.70  \n",
       "459      -1.000000    -0.937075        -0.989292 -0.341772          12.60  \n",
       "460      -0.994975    -0.903061        -0.992898 -0.518987          12.65  \n",
       "...            ...          ...              ...       ...            ...  \n",
       "1004      0.391960    -0.191327        -0.786167 -0.037975          20.13  \n",
       "1005      0.388610    -0.185374        -0.790319 -0.037975          20.15  \n",
       "1006      0.413735    -0.182823        -0.728912 -0.037975          20.78  \n",
       "1007      0.480737    -0.199830        -0.728038 -0.113924          20.15  \n",
       "1008      0.463987    -0.227041        -0.732955 -0.088608          20.38  \n",
       "1009      0.463987    -0.235544        -0.735686  0.037975          19.98  \n",
       "1010      0.460637    -0.230442        -0.730332 -0.113924          20.42  \n",
       "1011      0.460637    -0.179422        -0.724432  0.240506          20.42  \n",
       "1012      0.497487    -0.181122        -0.713505  0.341772          20.70  \n",
       "1013      0.541039    -0.147109        -0.704764  0.443038          20.60  \n",
       "1014      0.541039    -0.095238        -0.703671  0.316456          20.90  \n",
       "1015      0.624791    -0.092687        -0.697443  0.037975          21.25  \n",
       "1016      0.643216    -0.092687        -0.692635  0.164557          21.53  \n",
       "1017      0.643216    -0.119048        -0.678977  0.215190          21.38  \n",
       "1018      0.673367    -0.045918        -0.684113  0.316456          21.75  \n",
       "1019      0.708543    -0.035714        -0.672640  0.088608          22.30  \n",
       "1020      0.708543    -0.039966        -0.675809 -0.012658          22.08  \n",
       "1021      0.681742    -0.013605        -0.675262  0.189873          22.23  \n",
       "1022      0.698492     0.019558        -0.660948  0.341772          22.80  \n",
       "1023      0.644891     0.024660        -0.659965  0.341772          22.80  \n",
       "1024      0.651591     0.048469        -0.658108  0.265823          22.70  \n",
       "1025      0.631491     0.023810        -0.665210  0.392405          21.80  \n",
       "1026      0.690117     0.049320        -0.632430  0.468354          22.60  \n",
       "1027      0.690117     0.010204        -0.613418  0.367089          22.75  \n",
       "1028      0.778894    -0.068878        -0.649585  0.392405          22.45  \n",
       "1029      0.738693    -0.036565        -0.669580  0.367089          21.85  \n",
       "1030      0.785595     0.034864        -0.650240  0.240506          22.15  \n",
       "1031      0.773869     0.004252        -0.670673  0.240506          21.85  \n",
       "1032      0.731323     0.017857        -0.679414  0.113924          21.60  \n",
       "1033      0.757119     0.023810        -0.676901  0.164557          21.75  \n",
       "\n",
       "[3951 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('data(NCG).csv',sep=\";\")\n",
    "data = data[431:]\n",
    "data = data.append(pd.read_csv('data(NCG).csv',sep=\";\")[431:])\n",
    "data = data.append(pd.read_csv('data(PEG).csv',sep=\";\")[431:])\n",
    "data = data.append(pd.read_csv('data(VTP).csv',sep=\";\")[431:])\n",
    "data = data.append(pd.read_csv('data(ZEE).csv',sep=\";\")[431:])\n",
    "data = data.append(pd.read_csv('data.csv',sep=\";\")[431:])\n",
    "data = data.dropna()\n",
    "# display(data)\n",
    "# print(data['% FULL'].mean())\n",
    "\n",
    "scaler1 = MinMaxScaler(feature_range=(-1, 1))\n",
    "data['% FULL']=scaler1.fit_transform(data['% FULL'].values.reshape(-1, 1))\n",
    "scaler2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "data['Brent (eur/bbl)']=scaler2.fit_transform(data['Brent (eur/bbl)'].values.reshape(-1, 1))\n",
    "scaler3 = MinMaxScaler(feature_range=(-1, 1))\n",
    "data['Coal (eur/t)']=scaler3.fit_transform(data['Coal (eur/t)'].values.reshape(-1, 1))\n",
    "scaler4 = MinMaxScaler(feature_range=(-1, 1))\n",
    "data['CO2 (eur/t)']=scaler4.fit_transform(data['CO2 (eur/t)'].values.reshape(-1, 1))\n",
    "scaler5 = MinMaxScaler(feature_range=(-1, 1))\n",
    "data['month']=scaler5.fit_transform(data['month'].values.reshape(-1, 1))\n",
    "scaler6 = MinMaxScaler(feature_range=(-1, 1))\n",
    "data['JKM (Eur/mmbtu)']=scaler6.fit_transform(data['JKM (Eur/mmbtu)'].values.reshape(-1, 1))\n",
    "scaler7 = MinMaxScaler(feature_range=(-1, 1))\n",
    "data['Endex (eur/Mwh)']=scaler7.fit_transform(data['Endex (eur/Mwh)'].values.reshape(-1, 1))\n",
    "scaler8 = MinMaxScaler(feature_range=(-1, 1))\n",
    "data['AVG_TEMP']=scaler8.fit_transform(data['AVG_TEMP'].values.reshape(-1, 1))\n",
    "\n",
    "data_train = data.iloc[:int(data.shape[0]*0.95)]\n",
    "data_test = data.iloc[int(1+data.shape[0]*0.95):]\n",
    "display(data_train)\n",
    "data_train = shuffle(data_train)\n",
    "\n",
    "X_train= torch.Tensor(data_train[['month','% FULL','Brent (eur/bbl)','JKM (Eur/mmbtu)','Coal (eur/t)','CO2 (eur/t)','Endex (eur/Mwh)','AVG_TEMP']].astype(np.float32).values)\n",
    "y_train= torch.tensor(data_train['TTF (eur/Mwh)'].astype(np.float32).values)\n",
    "\n",
    "X_test= torch.Tensor(data_test[['month','% FULL','Brent (eur/bbl)','JKM (Eur/mmbtu)','Coal (eur/t)','CO2 (eur/t)','Endex (eur/Mwh)','AVG_TEMP']].astype(np.float32).values)\n",
    "y_test= torch.tensor(data_test['TTF (eur/Mwh)'].astype(np.float32).values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size,hidden3_size, hidden4_size, hidden5_size, hidden6_size, hidden7_size,hidden8_size, hidden9_size, hidden10_size,hidden11_size,num_classes):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden2_size, hidden3_size)\n",
    "        self.drop3 = nn.Dropout(p=0.5)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden3_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden3_size, hidden4_size)  \n",
    "        self.drop4 = nn.Dropout(p=0.5)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden4_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hidden4_size, hidden5_size)  \n",
    "        self.drop5 = nn.Dropout(p=0.5)\n",
    "        self.bn5 = nn.BatchNorm1d(hidden5_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.fc6 = nn.Linear(hidden5_size, hidden6_size)\n",
    "        self.drop6 = nn.Dropout(p=0.5)\n",
    "        self.bn6 = nn.BatchNorm1d(hidden6_size)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        \n",
    "        self.fc7 = nn.Linear(hidden6_size, hidden7_size)\n",
    "        self.drop7 = nn.Dropout(p=0.5)\n",
    "        self.bn7 = nn.BatchNorm1d(hidden2_size)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        \n",
    "        self.fc8 = nn.Linear(hidden7_size, hidden8_size)\n",
    "        self.drop8 = nn.Dropout(p=0.5)\n",
    "        self.bn8 = nn.BatchNorm1d(hidden8_size)\n",
    "        self.relu8 = nn.ReLU()\n",
    "        \n",
    "        self.fc9 = nn.Linear(hidden8_size, hidden9_size)  \n",
    "        self.drop9 = nn.Dropout(p=0.5)\n",
    "        self.bn9 = nn.BatchNorm1d(hidden9_size)\n",
    "        self.relu9 = nn.ReLU()\n",
    "        \n",
    "        self.fc10 = nn.Linear(hidden9_size, hidden10_size)  \n",
    "        self.drop10 = nn.Dropout(p=0.5)\n",
    "        self.bn10 = nn.BatchNorm1d(hidden10_size)\n",
    "        self.relu10 = nn.ReLU()\n",
    "        \n",
    "        self.fc11 = nn.Linear(hidden10_size, hidden11_size)  \n",
    "        self.drop11 = nn.Dropout(p=0.5)\n",
    "        self.bn11 = nn.BatchNorm1d(hidden11_size)\n",
    "        self.relu11 = nn.ReLU()\n",
    "        \n",
    "        self.fc12 = nn.Linear(hidden11_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.drop1(out)\n",
    "        out = self.relu1(out)\n",
    "        out= self.bn1(out)        \n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.drop2(out)\n",
    "        out = self.relu2(out)\n",
    "        out= self.bn2(out)         \n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.drop3(out)\n",
    "        out = self.relu3(out)\n",
    "        out= self.bn3(out)        \n",
    "        \n",
    "        out = self.fc4(out)\n",
    "        out = self.drop4(out)\n",
    "        out = self.relu4(out)\n",
    "#         out= self.bn4(out)        \n",
    "        \n",
    "        out = self.fc5(out)\n",
    "#         out = self.drop5(out)\n",
    "        out = self.relu5(out)\n",
    "#         out= self.bn5(out)       \n",
    "\n",
    "        out = self.fc6(out)\n",
    "#         out = self.drop6(out)\n",
    "        out = self.relu6(out)\n",
    "#         out= self.bn6(out)        \n",
    "        \n",
    "        out = self.fc7(out)\n",
    "#         out = self.drop7(out)\n",
    "        out = self.relu7(out)\n",
    "#         out= self.bn7(out)         \n",
    "        \n",
    "        out = self.fc8(out)\n",
    "#         out = self.drop8(out)\n",
    "        out = self.relu8(out)\n",
    "#         out= self.bn8(out)        \n",
    "        \n",
    "        out = self.fc9(out)\n",
    "#         out = self.drop9(out)\n",
    "        out = self.relu9(out)\n",
    "#         out= self.bn9(out)        \n",
    "        \n",
    "        out = self.fc10(out)\n",
    "#         out = self.drop10(out)\n",
    "        out = self.relu10(out)\n",
    "#         out= self.bn10(out)  \n",
    "\n",
    "        out = self.fc11(out)\n",
    "#         out = self.drop10(out)\n",
    "        out = self.relu11(out)\n",
    "#         out= self.bn10(out)  \n",
    "        \n",
    "        out = self.fc12(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't load model\n",
      "1 tensor(516.2654, device='cuda:1') tensor(361.5253, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "1001 tensor(516.1260, device='cuda:1') tensor(361.4108, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "2001 tensor(515.9598, device='cuda:1') tensor(361.2742, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "3001 tensor(515.7875, device='cuda:1') tensor(361.1325, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "4001 tensor(515.6131, device='cuda:1') tensor(360.9891, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "5001 tensor(515.4387, device='cuda:1') tensor(360.8457, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "6001 tensor(515.2648, device='cuda:1') tensor(360.7029, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "7001 tensor(515.0908, device='cuda:1') tensor(360.5597, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "8001 tensor(514.9164, device='cuda:1') tensor(360.4164, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "9001 tensor(514.7416, device='cuda:1') tensor(360.2728, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "10001 tensor(514.5670, device='cuda:1') tensor(360.1293, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "11001 tensor(514.3928, device='cuda:1') tensor(359.9861, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "12001 tensor(514.2189, device='cuda:1') tensor(359.8431, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "13001 tensor(514.0457, device='cuda:1') tensor(359.7009, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "14001 tensor(513.8726, device='cuda:1') tensor(359.5585, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "15001 tensor(513.6981, device='cuda:1') tensor(359.4153, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "16001 tensor(513.5226, device='cuda:1') tensor(359.2710, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "17001 tensor(513.3466, device='cuda:1') tensor(359.1264, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "18001 tensor(513.1703, device='cuda:1') tensor(358.9816, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "19001 tensor(512.9946, device='cuda:1') tensor(358.8372, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "20001 tensor(512.8206, device='cuda:1') tensor(358.6942, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "21001 tensor(512.6471, device='cuda:1') tensor(358.5518, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "22001 tensor(512.4731, device='cuda:1') tensor(358.4090, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "23001 tensor(512.2983, device='cuda:1') tensor(358.2653, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "24001 tensor(512.1230, device='cuda:1') tensor(358.1214, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "25001 tensor(511.9456, device='cuda:1') tensor(357.9756, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "26001 tensor(511.7694, device='cuda:1') tensor(357.8310, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "27001 tensor(511.5930, device='cuda:1') tensor(357.6860, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "28001 tensor(511.4158, device='cuda:1') tensor(357.5405, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "29001 tensor(511.2366, device='cuda:1') tensor(357.3934, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "30001 tensor(511.0582, device='cuda:1') tensor(357.2467, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "31001 tensor(510.8790, device='cuda:1') tensor(357.0996, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "32001 tensor(510.6994, device='cuda:1') tensor(356.9521, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "33001 tensor(510.5186, device='cuda:1') tensor(356.8038, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "34001 tensor(510.3370, device='cuda:1') tensor(356.6546, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "35001 tensor(510.1548, device='cuda:1') tensor(356.5050, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "36001 tensor(509.9716, device='cuda:1') tensor(356.3546, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "37001 tensor(509.7868, device='cuda:1') tensor(356.2029, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "38001 tensor(509.6012, device='cuda:1') tensor(356.0506, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "39001 tensor(509.4148, device='cuda:1') tensor(355.8975, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "40001 tensor(509.2265, device='cuda:1') tensor(355.7429, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "41001 tensor(509.0366, device='cuda:1') tensor(355.5871, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "42001 tensor(508.8459, device='cuda:1') tensor(355.4306, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "43001 tensor(508.6537, device='cuda:1') tensor(355.2729, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "44001 tensor(508.4592, device='cuda:1') tensor(355.1133, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "45001 tensor(508.2636, device='cuda:1') tensor(354.9527, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "46001 tensor(508.0665, device='cuda:1') tensor(354.7910, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "47001 tensor(507.8669, device='cuda:1') tensor(354.6270, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "48001 tensor(507.6636, device='cuda:1') tensor(354.4603, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "49001 tensor(507.4571, device='cuda:1') tensor(354.2908, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "50001 tensor(507.2483, device='cuda:1') tensor(354.1196, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "51001 tensor(507.0366, device='cuda:1') tensor(353.9459, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "52001 tensor(506.8218, device='cuda:1') tensor(353.7696, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "53001 tensor(506.6035, device='cuda:1') tensor(353.5905, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "54001 tensor(506.3843, device='cuda:1') tensor(353.4107, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "55001 tensor(506.1623, device='cuda:1') tensor(353.2285, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "56001 tensor(505.9378, device='cuda:1') tensor(353.0445, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "57001 tensor(505.7097, device='cuda:1') tensor(352.8574, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "58001 tensor(505.4776, device='cuda:1') tensor(352.6671, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "59001 tensor(505.2444, device='cuda:1') tensor(352.4758, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "60001 tensor(505.0089, device='cuda:1') tensor(352.2827, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "61001 tensor(504.7694, device='cuda:1') tensor(352.0862, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "62001 tensor(504.5269, device='cuda:1') tensor(351.8874, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "63001 tensor(504.2813, device='cuda:1') tensor(351.6860, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "64001 tensor(504.0312, device='cuda:1') tensor(351.4809, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "65001 tensor(503.7766, device='cuda:1') tensor(351.2721, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "66001 tensor(503.5171, device='cuda:1') tensor(351.0594, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "67001 tensor(503.2529, device='cuda:1') tensor(350.8428, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "68001 tensor(502.9843, device='cuda:1') tensor(350.6224, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "69001 tensor(502.7090, device='cuda:1') tensor(350.3968, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "70001 tensor(502.4294, device='cuda:1') tensor(350.1676, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "71001 tensor(502.1443, device='cuda:1') tensor(349.9340, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "72001 tensor(501.8535, device='cuda:1') tensor(349.6956, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "73001 tensor(501.5567, device='cuda:1') tensor(349.4524, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "74001 tensor(501.2556, device='cuda:1') tensor(349.2057, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "75001 tensor(500.9484, device='cuda:1') tensor(348.9540, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "76001 tensor(500.6352, device='cuda:1') tensor(348.6974, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "77001 tensor(500.3156, device='cuda:1') tensor(348.4355, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "78001 tensor(499.9903, device='cuda:1') tensor(348.1691, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "79001 tensor(499.6600, device='cuda:1') tensor(347.8985, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "80001 tensor(499.3236, device='cuda:1') tensor(347.6230, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "81001 tensor(498.9809, device='cuda:1') tensor(347.3422, device='cuda:1', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82001 tensor(498.6308, device='cuda:1') tensor(347.0555, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "83001 tensor(498.2734, device='cuda:1') tensor(346.7628, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "84001 tensor(497.9079, device='cuda:1') tensor(346.4636, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "85001 tensor(497.5345, device='cuda:1') tensor(346.1577, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "86001 tensor(497.1530, device='cuda:1') tensor(345.8454, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "87001 tensor(496.7633, device='cuda:1') tensor(345.5263, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "88001 tensor(496.3657, device='cuda:1') tensor(345.2008, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "89001 tensor(495.9592, device='cuda:1') tensor(344.8682, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "90001 tensor(495.5443, device='cuda:1') tensor(344.5285, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "91001 tensor(495.1202, device='cuda:1') tensor(344.1814, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "92001 tensor(494.6866, device='cuda:1') tensor(343.8267, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "93001 tensor(494.2432, device='cuda:1') tensor(343.4639, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "94001 tensor(493.7892, device='cuda:1') tensor(343.0925, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "95001 tensor(493.3251, device='cuda:1') tensor(342.7128, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "96001 tensor(492.8507, device='cuda:1') tensor(342.3248, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "97001 tensor(492.3655, device='cuda:1') tensor(341.9279, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "98001 tensor(491.8693, device='cuda:1') tensor(341.5222, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "99001 tensor(491.3616, device='cuda:1') tensor(341.1071, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "100001 tensor(490.8422, device='cuda:1') tensor(340.6823, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "101001 tensor(490.3106, device='cuda:1') tensor(340.2479, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "102001 tensor(489.7665, device='cuda:1') tensor(339.8031, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "103001 tensor(489.2095, device='cuda:1') tensor(339.3479, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "104001 tensor(488.6385, device='cuda:1') tensor(338.8813, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "105001 tensor(488.0542, device='cuda:1') tensor(338.4039, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "106001 tensor(487.4561, device='cuda:1') tensor(337.9152, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "107001 tensor(486.8436, device='cuda:1') tensor(337.4149, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "108001 tensor(486.2169, device='cuda:1') tensor(336.9030, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "109001 tensor(485.5753, device='cuda:1') tensor(336.3792, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "110001 tensor(484.9177, device='cuda:1') tensor(335.8422, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "111001 tensor(484.2431, device='cuda:1') tensor(335.2915, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "112001 tensor(483.5510, device='cuda:1') tensor(334.7266, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "113001 tensor(482.8423, device='cuda:1') tensor(334.1481, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "114001 tensor(482.1157, device='cuda:1') tensor(333.5553, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "115001 tensor(481.3711, device='cuda:1') tensor(332.9479, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "116001 tensor(480.6084, device='cuda:1') tensor(332.3257, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "117001 tensor(479.8271, device='cuda:1') tensor(331.6886, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "118001 tensor(479.0247, device='cuda:1') tensor(331.0342, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "119001 tensor(478.1990, device='cuda:1') tensor(330.3611, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "120001 tensor(477.3526, device='cuda:1') tensor(329.6712, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "121001 tensor(476.4855, device='cuda:1') tensor(328.9646, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "122001 tensor(475.5972, device='cuda:1') tensor(328.2409, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "123001 tensor(474.6869, device='cuda:1') tensor(327.4994, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "124001 tensor(473.7540, device='cuda:1') tensor(326.7397, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "125001 tensor(472.7979, device='cuda:1') tensor(325.9612, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "126001 tensor(471.8148, device='cuda:1') tensor(325.1609, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "127001 tensor(470.8066, device='cuda:1') tensor(324.3404, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "128001 tensor(469.7725, device='cuda:1') tensor(323.4990, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "129001 tensor(468.7125, device='cuda:1') tensor(322.6366, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "130001 tensor(467.6249, device='cuda:1') tensor(321.7521, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "131001 tensor(466.5098, device='cuda:1') tensor(320.8455, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "132001 tensor(465.3651, device='cuda:1') tensor(319.9150, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "133001 tensor(464.1909, device='cuda:1') tensor(318.9608, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "134001 tensor(462.9865, device='cuda:1') tensor(317.9825, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "135001 tensor(461.7512, device='cuda:1') tensor(316.9793, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "136001 tensor(460.4842, device='cuda:1') tensor(315.9507, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "137001 tensor(459.1841, device='cuda:1') tensor(314.8954, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "138001 tensor(457.8484, device='cuda:1') tensor(313.8117, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "139001 tensor(456.4764, device='cuda:1') tensor(312.6989, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "140001 tensor(455.0682, device='cuda:1') tensor(311.5572, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "141001 tensor(453.6232, device='cuda:1') tensor(310.3860, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "142001 tensor(452.1373, device='cuda:1') tensor(309.1819, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "143001 tensor(450.6113, device='cuda:1') tensor(307.9457, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "144001 tensor(449.0452, device='cuda:1') tensor(306.6775, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "145001 tensor(447.4354, device='cuda:1') tensor(305.3745, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "146001 tensor(445.7825, device='cuda:1') tensor(304.0372, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "147001 tensor(444.0849, device='cuda:1') tensor(302.6641, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "148001 tensor(442.3377, device='cuda:1') tensor(301.2517, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "149001 tensor(440.5423, device='cuda:1') tensor(299.8009, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "150001 tensor(438.6977, device='cuda:1') tensor(298.3111, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "151001 tensor(436.8031, device='cuda:1') tensor(296.7816, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "152001 tensor(434.8569, device='cuda:1') tensor(295.2111, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "153001 tensor(432.8572, device='cuda:1') tensor(293.5983, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "154001 tensor(430.7997, device='cuda:1') tensor(291.9397, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "155001 tensor(428.6844, device='cuda:1') tensor(290.2353, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "156001 tensor(426.5104, device='cuda:1') tensor(288.4848, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "157001 tensor(424.2760, device='cuda:1') tensor(286.6867, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "158001 tensor(421.9755, device='cuda:1') tensor(284.8365, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "159001 tensor(419.6096, device='cuda:1') tensor(282.9350, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "160001 tensor(417.1758, device='cuda:1') tensor(280.9803, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "161001 tensor(414.6738, device='cuda:1') tensor(278.9720, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "162001 tensor(412.1003, device='cuda:1') tensor(276.9079, device='cuda:1', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163001 tensor(409.4521, device='cuda:1') tensor(274.7854, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "164001 tensor(406.7278, device='cuda:1') tensor(272.6033, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "165001 tensor(403.9252, device='cuda:1') tensor(270.3603, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "166001 tensor(401.0417, device='cuda:1') tensor(268.0544, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "167001 tensor(398.0723, device='cuda:1') tensor(265.6819, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "168001 tensor(395.0147, device='cuda:1') tensor(263.2415, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "169001 tensor(391.8660, device='cuda:1') tensor(260.7309, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "170001 tensor(388.6233, device='cuda:1') tensor(258.1475, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "171001 tensor(385.2799, device='cuda:1') tensor(255.4862, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "172001 tensor(381.8361, device='cuda:1') tensor(252.7482, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "173001 tensor(378.2882, device='cuda:1') tensor(249.9306, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "174001 tensor(374.6317, device='cuda:1') tensor(247.0299, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "175001 tensor(370.8630, device='cuda:1') tensor(244.0440, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "176001 tensor(366.9790, device='cuda:1') tensor(240.9702, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "177001 tensor(362.9753, device='cuda:1') tensor(237.8057, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "178001 tensor(358.8469, device='cuda:1') tensor(234.5469, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "179001 tensor(354.5870, device='cuda:1') tensor(231.1895, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "180001 tensor(350.1901, device='cuda:1') tensor(227.7295, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "181001 tensor(345.6489, device='cuda:1') tensor(224.1618, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "182001 tensor(340.9616, device='cuda:1') tensor(220.4854, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "183001 tensor(336.1205, device='cuda:1') tensor(216.6958, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "184001 tensor(331.1202, device='cuda:1') tensor(212.7887, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "185001 tensor(325.9539, device='cuda:1') tensor(208.7599, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "186001 tensor(320.6156, device='cuda:1') tensor(204.6060, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "187001 tensor(315.0982, device='cuda:1') tensor(200.3224, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "188001 tensor(309.3919, device='cuda:1') tensor(195.9032, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "189001 tensor(303.4885, device='cuda:1') tensor(191.3433, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "190001 tensor(297.3810, device='cuda:1') tensor(186.6389, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "191001 tensor(291.0610, device='cuda:1') tensor(181.7854, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "192001 tensor(284.5158, device='cuda:1') tensor(176.7757, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "193001 tensor(277.7370, device='cuda:1') tensor(171.6058, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "194001 tensor(270.7168, device='cuda:1') tensor(166.2733, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "195001 tensor(263.4471, device='cuda:1') tensor(160.7755, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "196001 tensor(255.9171, device='cuda:1') tensor(155.1076, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "197001 tensor(248.1146, device='cuda:1') tensor(149.2653, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "198001 tensor(240.0292, device='cuda:1') tensor(143.2464, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "199001 tensor(231.6435, device='cuda:1') tensor(137.0448, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "200001 tensor(222.9487, device='cuda:1') tensor(130.6632, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "201001 tensor(213.9362, device='cuda:1') tensor(124.1040, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "202001 tensor(204.6047, device='cuda:1') tensor(117.3767, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "203001 tensor(194.9462, device='cuda:1') tensor(110.4887, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "204001 tensor(184.9611, device='cuda:1') tensor(103.4555, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "205001 tensor(174.6501, device='cuda:1') tensor(96.2946, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "206001 tensor(164.0183, device='cuda:1') tensor(89.0312, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "207001 tensor(153.0747, device='cuda:1') tensor(81.6967, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "208001 tensor(141.8358, device='cuda:1') tensor(74.3342, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "209001 tensor(130.3287, device='cuda:1') tensor(66.9986, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "210001 tensor(118.5972, device='cuda:1') tensor(59.7636, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "211001 tensor(106.6976, device='cuda:1') tensor(52.7206, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "212001 tensor(94.7032, device='cuda:1') tensor(45.9799, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "213001 tensor(82.6995, device='cuda:1') tensor(39.6718, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "214001 tensor(70.7981, device='cuda:1') tensor(33.9542, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "215001 tensor(59.1570, device='cuda:1') tensor(29.0122, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "216001 tensor(47.9759, device='cuda:1') tensor(25.0357, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "217001 tensor(37.5701, device='cuda:1') tensor(22.1530, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "218001 tensor(28.9620, device='cuda:1') tensor(20.2443, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "219001 tensor(24.2246, device='cuda:1') tensor(18.8985, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "220001 tensor(21.5238, device='cuda:1') tensor(17.7681, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "221001 tensor(19.2803, device='cuda:1') tensor(16.7095, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "222001 tensor(17.1996, device='cuda:1') tensor(15.6840, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "223001 tensor(15.2291, device='cuda:1') tensor(14.6842, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "224001 tensor(13.3573, device='cuda:1') tensor(13.7094, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "225001 tensor(11.5840, device='cuda:1') tensor(12.7647, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "226001 tensor(9.9233, device='cuda:1') tensor(11.8560, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "227001 tensor(8.3904, device='cuda:1') tensor(10.9913, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "228001 tensor(7.0044, device='cuda:1') tensor(10.1793, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "229001 tensor(5.7788, device='cuda:1') tensor(9.4289, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "230001 tensor(4.7242, device='cuda:1') tensor(8.7489, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "231001 tensor(3.8489, device='cuda:1') tensor(8.1470, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "232001 tensor(3.1575, device='cuda:1') tensor(7.6303, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "233001 tensor(2.6459, device='cuda:1') tensor(7.2020, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "234001 tensor(2.2972, device='cuda:1') tensor(6.8605, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "235001 tensor(2.0743, device='cuda:1') tensor(6.5968, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "236001 tensor(1.9234, device='cuda:1') tensor(6.3932, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "237001 tensor(1.7970, device='cuda:1') tensor(6.2299, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "238001 tensor(1.6772, device='cuda:1') tensor(6.0906, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "239001 tensor(1.5648, device='cuda:1') tensor(5.9655, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "240001 tensor(1.4623, device='cuda:1') tensor(5.8494, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "241001 tensor(1.3712, device='cuda:1') tensor(5.7396, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "242001 tensor(1.2926, device='cuda:1') tensor(5.6349, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "243001 tensor(1.2263, device='cuda:1') tensor(5.5341, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "244001 tensor(1.1713, device='cuda:1') tensor(5.4368, device='cuda:1', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245001 tensor(1.1252, device='cuda:1') tensor(5.3423, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "246001 tensor(1.0859, device='cuda:1') tensor(5.2500, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "247001 tensor(1.0522, device='cuda:1') tensor(5.1596, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "248001 tensor(1.0220, device='cuda:1') tensor(5.0710, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "249001 tensor(0.9947, device='cuda:1') tensor(4.9837, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "250001 tensor(0.9695, device='cuda:1') tensor(4.8973, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "251001 tensor(0.9459, device='cuda:1') tensor(4.8120, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "252001 tensor(0.9225, device='cuda:1') tensor(4.7277, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "253001 tensor(0.8995, device='cuda:1') tensor(4.6443, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "254001 tensor(0.8768, device='cuda:1') tensor(4.5616, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "255001 tensor(0.8547, device='cuda:1') tensor(4.4796, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "256001 tensor(0.8335, device='cuda:1') tensor(4.3982, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "257001 tensor(0.8116, device='cuda:1') tensor(4.3174, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "258001 tensor(0.7897, device='cuda:1') tensor(4.2368, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "259001 tensor(0.7681, device='cuda:1') tensor(4.1562, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "260001 tensor(0.7464, device='cuda:1') tensor(4.0755, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "261001 tensor(0.7251, device='cuda:1') tensor(3.9945, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "262001 tensor(0.7040, device='cuda:1') tensor(3.9128, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "263001 tensor(0.6837, device='cuda:1') tensor(3.8302, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "264001 tensor(0.6636, device='cuda:1') tensor(3.7463, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "265001 tensor(0.6438, device='cuda:1') tensor(3.6614, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "266001 tensor(0.6235, device='cuda:1') tensor(3.5755, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "267001 tensor(0.6030, device='cuda:1') tensor(3.4880, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "268001 tensor(0.5829, device='cuda:1') tensor(3.3997, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "269001 tensor(0.5646, device='cuda:1') tensor(3.3103, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "270001 tensor(0.5479, device='cuda:1') tensor(3.2198, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "271001 tensor(0.5327, device='cuda:1') tensor(3.1283, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "272001 tensor(0.5183, device='cuda:1') tensor(3.0360, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "273001 tensor(0.5041, device='cuda:1') tensor(2.9438, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "274001 tensor(0.4897, device='cuda:1') tensor(2.8519, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "275001 tensor(0.4755, device='cuda:1') tensor(2.7602, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "276001 tensor(0.4617, device='cuda:1') tensor(2.6691, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "277001 tensor(0.4485, device='cuda:1') tensor(2.5793, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "278001 tensor(0.4353, device='cuda:1') tensor(2.4910, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "279001 tensor(0.4237, device='cuda:1') tensor(2.4040, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "280001 tensor(0.4135, device='cuda:1') tensor(2.3183, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "281001 tensor(0.4032, device='cuda:1') tensor(2.2341, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "282001 tensor(0.3935, device='cuda:1') tensor(2.1520, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "283001 tensor(0.3840, device='cuda:1') tensor(2.0732, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "284001 tensor(0.3752, device='cuda:1') tensor(1.9982, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "285001 tensor(0.3669, device='cuda:1') tensor(1.9276, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "286001 tensor(0.3569, device='cuda:1') tensor(1.8617, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "287001 tensor(0.3473, device='cuda:1') tensor(1.8006, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "288001 tensor(0.3389, device='cuda:1') tensor(1.7445, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "289001 tensor(0.3307, device='cuda:1') tensor(1.6937, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "290001 tensor(0.3230, device='cuda:1') tensor(1.6478, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "291001 tensor(0.3164, device='cuda:1') tensor(1.6067, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "292001 tensor(0.3099, device='cuda:1') tensor(1.5697, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "293001 tensor(0.3025, device='cuda:1') tensor(1.5366, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "294001 tensor(0.2956, device='cuda:1') tensor(1.5074, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "295001 tensor(0.2886, device='cuda:1') tensor(1.4817, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "296001 tensor(0.2821, device='cuda:1') tensor(1.4587, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "297001 tensor(0.2774, device='cuda:1') tensor(1.4380, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "298001 tensor(0.2734, device='cuda:1') tensor(1.4188, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "299001 tensor(0.2692, device='cuda:1') tensor(1.4011, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "300001 tensor(0.2654, device='cuda:1') tensor(1.3850, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "301001 tensor(0.2623, device='cuda:1') tensor(1.3700, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "302001 tensor(0.2598, device='cuda:1') tensor(1.3560, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "303001 tensor(0.2574, device='cuda:1') tensor(1.3421, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "304001 tensor(0.2551, device='cuda:1') tensor(1.3290, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "305001 tensor(0.2531, device='cuda:1') tensor(1.3163, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "306001 tensor(0.2505, device='cuda:1') tensor(1.3042, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "307001 tensor(0.2487, device='cuda:1') tensor(1.2929, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "308001 tensor(0.2473, device='cuda:1') tensor(1.2817, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "309001 tensor(0.2458, device='cuda:1') tensor(1.2700, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "310001 tensor(0.2445, device='cuda:1') tensor(1.2588, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "311001 tensor(0.2433, device='cuda:1') tensor(1.2482, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "312001 tensor(0.2420, device='cuda:1') tensor(1.2381, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "313001 tensor(0.2412, device='cuda:1') tensor(1.2278, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "314001 tensor(0.2404, device='cuda:1') tensor(1.2181, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "315001 tensor(0.2397, device='cuda:1') tensor(1.2086, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "316001 tensor(0.2388, device='cuda:1') tensor(1.1990, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "317001 tensor(0.2378, device='cuda:1') tensor(1.1895, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "318001 tensor(0.2371, device='cuda:1') tensor(1.1803, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "319001 tensor(0.2365, device='cuda:1') tensor(1.1711, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "320001 tensor(0.2362, device='cuda:1') tensor(1.1620, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "321001 tensor(0.2355, device='cuda:1') tensor(1.1527, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "322001 tensor(0.2349, device='cuda:1') tensor(1.1436, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "323001 tensor(0.2340, device='cuda:1') tensor(1.1345, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "324001 tensor(0.2333, device='cuda:1') tensor(1.1256, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "325001 tensor(0.2328, device='cuda:1') tensor(1.1168, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "326001 tensor(0.2322, device='cuda:1') tensor(1.1081, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "327001 tensor(0.2319, device='cuda:1') tensor(1.0993, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "328001 tensor(0.2315, device='cuda:1') tensor(1.0908, device='cuda:1', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329001 tensor(0.2310, device='cuda:1') tensor(1.0823, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "330001 tensor(0.2306, device='cuda:1') tensor(1.0738, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "331001 tensor(0.2302, device='cuda:1') tensor(1.0656, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "332001 tensor(0.2296, device='cuda:1') tensor(1.0574, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "333001 tensor(0.2289, device='cuda:1') tensor(1.0493, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "334001 tensor(0.2284, device='cuda:1') tensor(1.0413, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "335001 tensor(0.2279, device='cuda:1') tensor(1.0334, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "336001 tensor(0.2272, device='cuda:1') tensor(1.0253, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "337001 tensor(0.2262, device='cuda:1') tensor(1.0173, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "338001 tensor(0.2256, device='cuda:1') tensor(1.0088, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "339001 tensor(0.2247, device='cuda:1') tensor(1.0007, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "340001 tensor(0.2239, device='cuda:1') tensor(0.9928, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "341001 tensor(0.2232, device='cuda:1') tensor(0.9853, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "342001 tensor(0.2225, device='cuda:1') tensor(0.9777, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "343001 tensor(0.2212, device='cuda:1') tensor(0.9703, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "344001 tensor(0.2203, device='cuda:1') tensor(0.9629, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "345001 tensor(0.2193, device='cuda:1') tensor(0.9556, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "346001 tensor(0.2181, device='cuda:1') tensor(0.9483, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "347001 tensor(0.2174, device='cuda:1') tensor(0.9409, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "348001 tensor(0.2164, device='cuda:1') tensor(0.9337, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "349001 tensor(0.2152, device='cuda:1') tensor(0.9265, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "350001 tensor(0.2139, device='cuda:1') tensor(0.9196, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "351001 tensor(0.2128, device='cuda:1') tensor(0.9127, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "352001 tensor(0.2115, device='cuda:1') tensor(0.9059, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "353001 tensor(0.2104, device='cuda:1') tensor(0.8991, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "354001 tensor(0.2091, device='cuda:1') tensor(0.8924, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "355001 tensor(0.2078, device='cuda:1') tensor(0.8858, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "356001 tensor(0.2066, device='cuda:1') tensor(0.8792, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "357001 tensor(0.2054, device='cuda:1') tensor(0.8728, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "358001 tensor(0.2045, device='cuda:1') tensor(0.8665, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "359001 tensor(0.2042, device='cuda:1') tensor(0.8604, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "360001 tensor(0.2037, device='cuda:1') tensor(0.8543, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "361001 tensor(0.2031, device='cuda:1') tensor(0.8482, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "362001 tensor(0.2020, device='cuda:1') tensor(0.8422, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "363001 tensor(0.2009, device='cuda:1') tensor(0.8363, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "364001 tensor(0.2001, device='cuda:1') tensor(0.8305, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "365001 tensor(0.1993, device='cuda:1') tensor(0.8248, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "366001 tensor(0.1987, device='cuda:1') tensor(0.8191, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "367001 tensor(0.1979, device='cuda:1') tensor(0.8135, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "368001 tensor(0.1970, device='cuda:1') tensor(0.8079, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "369001 tensor(0.1966, device='cuda:1') tensor(0.8023, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "370001 tensor(0.1961, device='cuda:1') tensor(0.7968, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "371001 tensor(0.1956, device='cuda:1') tensor(0.7913, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "372001 tensor(0.1950, device='cuda:1') tensor(0.7859, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "373001 tensor(0.1947, device='cuda:1') tensor(0.7805, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "374001 tensor(0.1946, device='cuda:1') tensor(0.7751, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "375001 tensor(0.1942, device='cuda:1') tensor(0.7696, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "376001 tensor(0.1941, device='cuda:1') tensor(0.7643, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "377001 tensor(0.1939, device='cuda:1') tensor(0.7590, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "378001 tensor(0.1940, device='cuda:1') tensor(0.7537, device='cuda:1', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7096.445566892624\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=\"cpu\"\n",
    "num_nodes = 640\n",
    "net = Net(8,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,1).to(torch.device(device))  \n",
    "net.train()\n",
    "X_train = X_train.float().to(device)\n",
    "y_train = y_train.float().to(device)\n",
    "X_test = X_test.float().to(device)\n",
    "y_test = y_test.float().to(device)\n",
    "\n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load('yearly_model_all.pth'))   \n",
    "except:\n",
    "    print(\"can't load model\")\n",
    "# learning_rate = 0.0000001\n",
    "learning_rate = 0.0000001\n",
    "opt = optim.Adam(params=net.parameters(),lr=learning_rate)\n",
    "\n",
    "loss_train_arr = []\n",
    "loss_test_arr = []\n",
    "\n",
    "prev_loss = 100000000\n",
    "curr_loss = 999\n",
    "epoch = 0\n",
    "\n",
    "# for epoch in range(2500):\n",
    "while prev_loss >= curr_loss:\n",
    "    \n",
    "    #do the forward propagation\n",
    "    out_train = net(X_train)\n",
    "    out_test = net(X_test)\n",
    "\n",
    "    # Out loss function\n",
    "    loss_train = F.mse_loss(out_train.view(1, -1), y_train.float().view(1, -1))\n",
    "    loss_train_arr.append(loss_train.data)\n",
    "    loss_test = F.mse_loss(out_test.view(1, -1), y_test.float().view(1, -1)).data\n",
    "    loss_test_arr.append(loss_test)\n",
    "#     prev_loss = curr_loss\n",
    "#     curr_loss = loss_test\n",
    "\n",
    "    epoch = epoch + 1\n",
    "    if epoch % 1000 == 1:\n",
    "        print(epoch, loss_test,loss_train)\n",
    "        torch.save(net.state_dict(),'yearly_model_all.pth')\n",
    "        prev_loss = curr_loss\n",
    "        curr_loss = loss_test\n",
    "    # Our optimizer\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, nesterov=True, momentum=0.9, dampening=0)\n",
    "\n",
    "    #do back propagation\n",
    "    opt.zero_grad()\n",
    "    loss_train.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(net.parameters(), 0.00001)\n",
    "    torch.nn.utils.clip_grad_norm_(net.parameters(), 0.00001)     \n",
    "\n",
    "    opt.step()\n",
    "\n",
    "torch.save(net.state_dict(),'yearly_model_all.pth')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(loss_train_arr[200:])), loss_train_arr[200:],label=\"train\")\n",
    "plt.plot(np.arange(len(loss_test_arr[200:])), loss_test_arr[200:],label=\"test\")\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(loss_train_arr[2000:2500])), loss_train_arr[2000:2500])\n",
    "plt.plot(np.arange(len(loss_test_arr[2000:2500])), loss_test_arr[2000:2500])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_nodes = 32\n",
    "net = Net(7,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,1).to(torch.device(device))  \n",
    "try:\n",
    "    net.load_state_dict(torch.load('yearly_model_all.pth'))\n",
    "    net.eval()\n",
    "except:\n",
    "    print(\"can't load model\")\n",
    "learning_rate = 0.0000001\n",
    "opt = optim.Adam(params=net.parameters(),lr=learning_rate)\n",
    "\n",
    "X_test = X_test.float().to(device)  \n",
    "out = net(X_test).data\n",
    "out = out.cpu().data.numpy()\n",
    "print(out)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(out)), out)\n",
    "plt.plot(np.arange(len(out), y_test.data.numpy()))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>% FULL</th>\n",
       "      <th>Brent (eur/bbl)</th>\n",
       "      <th>JKM (Eur/mmbtu)</th>\n",
       "      <th>Coal (eur/t)</th>\n",
       "      <th>CO2 (eur/t)</th>\n",
       "      <th>Endex (eur/Mwh)</th>\n",
       "      <th>AVG_TEMP</th>\n",
       "      <th>current TTF_forward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>66.143875</td>\n",
       "      <td>62.252762</td>\n",
       "      <td>5.058093</td>\n",
       "      <td>63.35</td>\n",
       "      <td>25.13</td>\n",
       "      <td>43.19</td>\n",
       "      <td>14.939560</td>\n",
       "      <td>14.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>76.814960</td>\n",
       "      <td>61.666059</td>\n",
       "      <td>5.266994</td>\n",
       "      <td>64.90</td>\n",
       "      <td>25.14</td>\n",
       "      <td>42.58</td>\n",
       "      <td>18.146067</td>\n",
       "      <td>14.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>88.021452</td>\n",
       "      <td>60.522381</td>\n",
       "      <td>5.386510</td>\n",
       "      <td>66.85</td>\n",
       "      <td>25.17</td>\n",
       "      <td>48.50</td>\n",
       "      <td>17.863636</td>\n",
       "      <td>14.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>96.625125</td>\n",
       "      <td>59.656315</td>\n",
       "      <td>5.931612</td>\n",
       "      <td>68.80</td>\n",
       "      <td>25.20</td>\n",
       "      <td>53.24</td>\n",
       "      <td>16.028846</td>\n",
       "      <td>15.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>101.231129</td>\n",
       "      <td>58.821493</td>\n",
       "      <td>6.567448</td>\n",
       "      <td>69.85</td>\n",
       "      <td>25.30</td>\n",
       "      <td>55.11</td>\n",
       "      <td>12.274436</td>\n",
       "      <td>17.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>99.582292</td>\n",
       "      <td>57.618885</td>\n",
       "      <td>7.128856</td>\n",
       "      <td>70.50</td>\n",
       "      <td>25.35</td>\n",
       "      <td>55.90</td>\n",
       "      <td>8.689189</td>\n",
       "      <td>20.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>88.595645</td>\n",
       "      <td>56.510417</td>\n",
       "      <td>7.329469</td>\n",
       "      <td>71.20</td>\n",
       "      <td>25.41</td>\n",
       "      <td>56.20</td>\n",
       "      <td>6.783654</td>\n",
       "      <td>21.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>75.357097</td>\n",
       "      <td>55.513182</td>\n",
       "      <td>7.295639</td>\n",
       "      <td>72.00</td>\n",
       "      <td>25.47</td>\n",
       "      <td>56.90</td>\n",
       "      <td>4.069565</td>\n",
       "      <td>21.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>59.192845</td>\n",
       "      <td>55.272757</td>\n",
       "      <td>6.590947</td>\n",
       "      <td>72.55</td>\n",
       "      <td>25.60</td>\n",
       "      <td>55.33</td>\n",
       "      <td>4.152381</td>\n",
       "      <td>21.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>48.391089</td>\n",
       "      <td>55.015752</td>\n",
       "      <td>5.803349</td>\n",
       "      <td>72.40</td>\n",
       "      <td>25.83</td>\n",
       "      <td>49.75</td>\n",
       "      <td>5.921296</td>\n",
       "      <td>20.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>47.241833</td>\n",
       "      <td>54.750456</td>\n",
       "      <td>5.678992</td>\n",
       "      <td>72.35</td>\n",
       "      <td>26.29</td>\n",
       "      <td>46.94</td>\n",
       "      <td>8.418750</td>\n",
       "      <td>19.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>54.966290</td>\n",
       "      <td>54.518322</td>\n",
       "      <td>5.616813</td>\n",
       "      <td>72.25</td>\n",
       "      <td>26.82</td>\n",
       "      <td>45.45</td>\n",
       "      <td>12.776087</td>\n",
       "      <td>18.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>65.416375</td>\n",
       "      <td>54.269607</td>\n",
       "      <td>5.658266</td>\n",
       "      <td>72.30</td>\n",
       "      <td>27.35</td>\n",
       "      <td>45.63</td>\n",
       "      <td>14.939560</td>\n",
       "      <td>17.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>76.087460</td>\n",
       "      <td>54.020892</td>\n",
       "      <td>5.720444</td>\n",
       "      <td>72.20</td>\n",
       "      <td>27.88</td>\n",
       "      <td>46.52</td>\n",
       "      <td>18.146067</td>\n",
       "      <td>17.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>87.293952</td>\n",
       "      <td>53.763887</td>\n",
       "      <td>5.782623</td>\n",
       "      <td>72.35</td>\n",
       "      <td>27.88</td>\n",
       "      <td>49.88</td>\n",
       "      <td>17.863636</td>\n",
       "      <td>18.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>95.897625</td>\n",
       "      <td>53.498591</td>\n",
       "      <td>6.114243</td>\n",
       "      <td>72.50</td>\n",
       "      <td>27.88</td>\n",
       "      <td>52.54</td>\n",
       "      <td>16.028846</td>\n",
       "      <td>18.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>100.503629</td>\n",
       "      <td>53.241585</td>\n",
       "      <td>6.300779</td>\n",
       "      <td>72.30</td>\n",
       "      <td>27.88</td>\n",
       "      <td>53.58</td>\n",
       "      <td>12.274436</td>\n",
       "      <td>19.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>98.854792</td>\n",
       "      <td>53.042613</td>\n",
       "      <td>6.918421</td>\n",
       "      <td>72.50</td>\n",
       "      <td>27.88</td>\n",
       "      <td>54.36</td>\n",
       "      <td>8.689189</td>\n",
       "      <td>20.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>87.868145</td>\n",
       "      <td>52.843641</td>\n",
       "      <td>7.250041</td>\n",
       "      <td>73.15</td>\n",
       "      <td>27.88</td>\n",
       "      <td>53.21</td>\n",
       "      <td>6.783654</td>\n",
       "      <td>20.618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>74.629597</td>\n",
       "      <td>52.652960</td>\n",
       "      <td>7.125684</td>\n",
       "      <td>74.05</td>\n",
       "      <td>27.88</td>\n",
       "      <td>51.63</td>\n",
       "      <td>4.069565</td>\n",
       "      <td>21.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>58.700893</td>\n",
       "      <td>52.470569</td>\n",
       "      <td>6.876969</td>\n",
       "      <td>74.55</td>\n",
       "      <td>27.88</td>\n",
       "      <td>49.41</td>\n",
       "      <td>4.152381</td>\n",
       "      <td>20.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>48.066089</td>\n",
       "      <td>52.288178</td>\n",
       "      <td>6.172277</td>\n",
       "      <td>74.40</td>\n",
       "      <td>27.88</td>\n",
       "      <td>45.77</td>\n",
       "      <td>5.921296</td>\n",
       "      <td>19.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>46.465050</td>\n",
       "      <td>52.105787</td>\n",
       "      <td>5.720444</td>\n",
       "      <td>74.20</td>\n",
       "      <td>27.88</td>\n",
       "      <td>45.30</td>\n",
       "      <td>8.418750</td>\n",
       "      <td>19.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>46.465050</td>\n",
       "      <td>51.923396</td>\n",
       "      <td>5.513182</td>\n",
       "      <td>74.10</td>\n",
       "      <td>27.88</td>\n",
       "      <td>45.01</td>\n",
       "      <td>12.776087</td>\n",
       "      <td>18.193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month      % FULL  Brent (eur/bbl)  JKM (Eur/mmbtu)  Coal (eur/t)  \\\n",
       "0       6   66.143875        62.252762         5.058093         63.35   \n",
       "1       7   76.814960        61.666059         5.266994         64.90   \n",
       "2       8   88.021452        60.522381         5.386510         66.85   \n",
       "3       9   96.625125        59.656315         5.931612         68.80   \n",
       "4      10  101.231129        58.821493         6.567448         69.85   \n",
       "5      11   99.582292        57.618885         7.128856         70.50   \n",
       "6      12   88.595645        56.510417         7.329469         71.20   \n",
       "7       1   75.357097        55.513182         7.295639         72.00   \n",
       "8       2   59.192845        55.272757         6.590947         72.55   \n",
       "9       3   48.391089        55.015752         5.803349         72.40   \n",
       "10      4   47.241833        54.750456         5.678992         72.35   \n",
       "11      5   54.966290        54.518322         5.616813         72.25   \n",
       "12      6   65.416375        54.269607         5.658266         72.30   \n",
       "13      7   76.087460        54.020892         5.720444         72.20   \n",
       "14      8   87.293952        53.763887         5.782623         72.35   \n",
       "15      9   95.897625        53.498591         6.114243         72.50   \n",
       "16     10  100.503629        53.241585         6.300779         72.30   \n",
       "17     11   98.854792        53.042613         6.918421         72.50   \n",
       "18     12   87.868145        52.843641         7.250041         73.15   \n",
       "19      1   74.629597        52.652960         7.125684         74.05   \n",
       "20      2   58.700893        52.470569         6.876969         74.55   \n",
       "21      3   48.066089        52.288178         6.172277         74.40   \n",
       "22      4   46.465050        52.105787         5.720444         74.20   \n",
       "23      5   46.465050        51.923396         5.513182         74.10   \n",
       "\n",
       "    CO2 (eur/t)  Endex (eur/Mwh)   AVG_TEMP  current TTF_forward  \n",
       "0         25.13            43.19  14.939560               14.566  \n",
       "1         25.14            42.58  18.146067               14.688  \n",
       "2         25.17            48.50  17.863636               14.938  \n",
       "3         25.20            53.24  16.028846               15.713  \n",
       "4         25.30            55.11  12.274436               17.795  \n",
       "5         25.35            55.90   8.689189               20.490  \n",
       "6         25.41            56.20   6.783654               21.035  \n",
       "7         25.47            56.90   4.069565               21.130  \n",
       "8         25.60            55.33   4.152381               21.180  \n",
       "9         25.83            49.75   5.921296               20.759  \n",
       "10        26.29            46.94   8.418750               19.268  \n",
       "11        26.82            45.45  12.776087               18.358  \n",
       "12        27.35            45.63  14.939560               17.853  \n",
       "13        27.88            46.52  18.146067               17.828  \n",
       "14        27.88            49.88  17.863636               18.033  \n",
       "15        27.88            52.54  16.028846               18.738  \n",
       "16        27.88            53.58  12.274436               19.553  \n",
       "17        27.88            54.36   8.689189               20.158  \n",
       "18        27.88            53.21   6.783654               20.618  \n",
       "19        27.88            51.63   4.069565               21.138  \n",
       "20        27.88            49.41   4.152381               20.948  \n",
       "21        27.88            45.77   5.921296               19.968  \n",
       "22        27.88            45.30   8.418750               19.038  \n",
       "23        27.88            45.01  12.776087               18.193  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>% FULL</th>\n",
       "      <th>Brent (eur/bbl)</th>\n",
       "      <th>JKM (Eur/mmbtu)</th>\n",
       "      <th>Coal (eur/t)</th>\n",
       "      <th>CO2 (eur/t)</th>\n",
       "      <th>Endex (eur/Mwh)</th>\n",
       "      <th>AVG_TEMP</th>\n",
       "      <th>current TTF_forward</th>\n",
       "      <th>predicted TTF</th>\n",
       "      <th>market premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>66.143875</td>\n",
       "      <td>62.252762</td>\n",
       "      <td>5.058093</td>\n",
       "      <td>63.35</td>\n",
       "      <td>25.13</td>\n",
       "      <td>43.19</td>\n",
       "      <td>14.939560</td>\n",
       "      <td>14.566</td>\n",
       "      <td>16.749908</td>\n",
       "      <td>-2.183908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>76.814960</td>\n",
       "      <td>61.666059</td>\n",
       "      <td>5.266994</td>\n",
       "      <td>64.90</td>\n",
       "      <td>25.14</td>\n",
       "      <td>42.58</td>\n",
       "      <td>18.146067</td>\n",
       "      <td>14.688</td>\n",
       "      <td>17.100821</td>\n",
       "      <td>-2.412821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>88.021452</td>\n",
       "      <td>60.522381</td>\n",
       "      <td>5.386510</td>\n",
       "      <td>66.85</td>\n",
       "      <td>25.17</td>\n",
       "      <td>48.50</td>\n",
       "      <td>17.863636</td>\n",
       "      <td>14.938</td>\n",
       "      <td>17.150129</td>\n",
       "      <td>-2.212129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>96.625125</td>\n",
       "      <td>59.656315</td>\n",
       "      <td>5.931612</td>\n",
       "      <td>68.80</td>\n",
       "      <td>25.20</td>\n",
       "      <td>53.24</td>\n",
       "      <td>16.028846</td>\n",
       "      <td>15.713</td>\n",
       "      <td>18.106243</td>\n",
       "      <td>-2.393243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>101.231129</td>\n",
       "      <td>58.821493</td>\n",
       "      <td>6.567448</td>\n",
       "      <td>69.85</td>\n",
       "      <td>25.30</td>\n",
       "      <td>55.11</td>\n",
       "      <td>12.274436</td>\n",
       "      <td>17.795</td>\n",
       "      <td>19.850586</td>\n",
       "      <td>-2.055586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>99.582292</td>\n",
       "      <td>57.618885</td>\n",
       "      <td>7.128856</td>\n",
       "      <td>70.50</td>\n",
       "      <td>25.35</td>\n",
       "      <td>55.90</td>\n",
       "      <td>8.689189</td>\n",
       "      <td>20.490</td>\n",
       "      <td>22.440113</td>\n",
       "      <td>-1.950113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>88.595645</td>\n",
       "      <td>56.510417</td>\n",
       "      <td>7.329469</td>\n",
       "      <td>71.20</td>\n",
       "      <td>25.41</td>\n",
       "      <td>56.20</td>\n",
       "      <td>6.783654</td>\n",
       "      <td>21.035</td>\n",
       "      <td>22.264355</td>\n",
       "      <td>-1.229355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>75.357097</td>\n",
       "      <td>55.513182</td>\n",
       "      <td>7.295639</td>\n",
       "      <td>72.00</td>\n",
       "      <td>25.47</td>\n",
       "      <td>56.90</td>\n",
       "      <td>4.069565</td>\n",
       "      <td>21.130</td>\n",
       "      <td>19.772421</td>\n",
       "      <td>1.357579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>59.192845</td>\n",
       "      <td>55.272757</td>\n",
       "      <td>6.590947</td>\n",
       "      <td>72.55</td>\n",
       "      <td>25.60</td>\n",
       "      <td>55.33</td>\n",
       "      <td>4.152381</td>\n",
       "      <td>21.180</td>\n",
       "      <td>18.924877</td>\n",
       "      <td>2.255123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>48.391089</td>\n",
       "      <td>55.015752</td>\n",
       "      <td>5.803349</td>\n",
       "      <td>72.40</td>\n",
       "      <td>25.83</td>\n",
       "      <td>49.75</td>\n",
       "      <td>5.921296</td>\n",
       "      <td>20.759</td>\n",
       "      <td>17.766306</td>\n",
       "      <td>2.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>47.241833</td>\n",
       "      <td>54.750456</td>\n",
       "      <td>5.678992</td>\n",
       "      <td>72.35</td>\n",
       "      <td>26.29</td>\n",
       "      <td>46.94</td>\n",
       "      <td>8.418750</td>\n",
       "      <td>19.268</td>\n",
       "      <td>17.739880</td>\n",
       "      <td>1.528120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>54.966290</td>\n",
       "      <td>54.518322</td>\n",
       "      <td>5.616813</td>\n",
       "      <td>72.25</td>\n",
       "      <td>26.82</td>\n",
       "      <td>45.45</td>\n",
       "      <td>12.776087</td>\n",
       "      <td>18.358</td>\n",
       "      <td>19.005142</td>\n",
       "      <td>-0.647142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>65.416375</td>\n",
       "      <td>54.269607</td>\n",
       "      <td>5.658266</td>\n",
       "      <td>72.30</td>\n",
       "      <td>27.35</td>\n",
       "      <td>45.63</td>\n",
       "      <td>14.939560</td>\n",
       "      <td>17.853</td>\n",
       "      <td>20.176157</td>\n",
       "      <td>-2.323157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>76.087460</td>\n",
       "      <td>54.020892</td>\n",
       "      <td>5.720444</td>\n",
       "      <td>72.20</td>\n",
       "      <td>27.88</td>\n",
       "      <td>46.52</td>\n",
       "      <td>18.146067</td>\n",
       "      <td>17.828</td>\n",
       "      <td>19.939632</td>\n",
       "      <td>-2.111632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>87.293952</td>\n",
       "      <td>53.763887</td>\n",
       "      <td>5.782623</td>\n",
       "      <td>72.35</td>\n",
       "      <td>27.88</td>\n",
       "      <td>49.88</td>\n",
       "      <td>17.863636</td>\n",
       "      <td>18.033</td>\n",
       "      <td>19.520908</td>\n",
       "      <td>-1.487908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>95.897625</td>\n",
       "      <td>53.498591</td>\n",
       "      <td>6.114243</td>\n",
       "      <td>72.50</td>\n",
       "      <td>27.88</td>\n",
       "      <td>52.54</td>\n",
       "      <td>16.028846</td>\n",
       "      <td>18.738</td>\n",
       "      <td>19.902899</td>\n",
       "      <td>-1.164899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>100.503629</td>\n",
       "      <td>53.241585</td>\n",
       "      <td>6.300779</td>\n",
       "      <td>72.30</td>\n",
       "      <td>27.88</td>\n",
       "      <td>53.58</td>\n",
       "      <td>12.274436</td>\n",
       "      <td>19.553</td>\n",
       "      <td>21.136690</td>\n",
       "      <td>-1.583690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>98.854792</td>\n",
       "      <td>53.042613</td>\n",
       "      <td>6.918421</td>\n",
       "      <td>72.50</td>\n",
       "      <td>27.88</td>\n",
       "      <td>54.36</td>\n",
       "      <td>8.689189</td>\n",
       "      <td>20.158</td>\n",
       "      <td>23.004005</td>\n",
       "      <td>-2.846005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>87.868145</td>\n",
       "      <td>52.843641</td>\n",
       "      <td>7.250041</td>\n",
       "      <td>73.15</td>\n",
       "      <td>27.88</td>\n",
       "      <td>53.21</td>\n",
       "      <td>6.783654</td>\n",
       "      <td>20.618</td>\n",
       "      <td>22.561266</td>\n",
       "      <td>-1.943266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>74.629597</td>\n",
       "      <td>52.652960</td>\n",
       "      <td>7.125684</td>\n",
       "      <td>74.05</td>\n",
       "      <td>27.88</td>\n",
       "      <td>51.63</td>\n",
       "      <td>4.069565</td>\n",
       "      <td>21.138</td>\n",
       "      <td>21.507097</td>\n",
       "      <td>-0.369097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>58.700893</td>\n",
       "      <td>52.470569</td>\n",
       "      <td>6.876969</td>\n",
       "      <td>74.55</td>\n",
       "      <td>27.88</td>\n",
       "      <td>49.41</td>\n",
       "      <td>4.152381</td>\n",
       "      <td>20.948</td>\n",
       "      <td>19.827475</td>\n",
       "      <td>1.120525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>48.066089</td>\n",
       "      <td>52.288178</td>\n",
       "      <td>6.172277</td>\n",
       "      <td>74.40</td>\n",
       "      <td>27.88</td>\n",
       "      <td>45.77</td>\n",
       "      <td>5.921296</td>\n",
       "      <td>19.968</td>\n",
       "      <td>18.894464</td>\n",
       "      <td>1.073536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>46.465050</td>\n",
       "      <td>52.105787</td>\n",
       "      <td>5.720444</td>\n",
       "      <td>74.20</td>\n",
       "      <td>27.88</td>\n",
       "      <td>45.30</td>\n",
       "      <td>8.418750</td>\n",
       "      <td>19.038</td>\n",
       "      <td>18.800505</td>\n",
       "      <td>0.237495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>46.465050</td>\n",
       "      <td>51.923396</td>\n",
       "      <td>5.513182</td>\n",
       "      <td>74.10</td>\n",
       "      <td>27.88</td>\n",
       "      <td>45.01</td>\n",
       "      <td>12.776087</td>\n",
       "      <td>18.193</td>\n",
       "      <td>19.609299</td>\n",
       "      <td>-1.416299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month      % FULL  Brent (eur/bbl)  JKM (Eur/mmbtu)  Coal (eur/t)  \\\n",
       "0       6   66.143875        62.252762         5.058093         63.35   \n",
       "1       7   76.814960        61.666059         5.266994         64.90   \n",
       "2       8   88.021452        60.522381         5.386510         66.85   \n",
       "3       9   96.625125        59.656315         5.931612         68.80   \n",
       "4      10  101.231129        58.821493         6.567448         69.85   \n",
       "5      11   99.582292        57.618885         7.128856         70.50   \n",
       "6      12   88.595645        56.510417         7.329469         71.20   \n",
       "7       1   75.357097        55.513182         7.295639         72.00   \n",
       "8       2   59.192845        55.272757         6.590947         72.55   \n",
       "9       3   48.391089        55.015752         5.803349         72.40   \n",
       "10      4   47.241833        54.750456         5.678992         72.35   \n",
       "11      5   54.966290        54.518322         5.616813         72.25   \n",
       "12      6   65.416375        54.269607         5.658266         72.30   \n",
       "13      7   76.087460        54.020892         5.720444         72.20   \n",
       "14      8   87.293952        53.763887         5.782623         72.35   \n",
       "15      9   95.897625        53.498591         6.114243         72.50   \n",
       "16     10  100.503629        53.241585         6.300779         72.30   \n",
       "17     11   98.854792        53.042613         6.918421         72.50   \n",
       "18     12   87.868145        52.843641         7.250041         73.15   \n",
       "19      1   74.629597        52.652960         7.125684         74.05   \n",
       "20      2   58.700893        52.470569         6.876969         74.55   \n",
       "21      3   48.066089        52.288178         6.172277         74.40   \n",
       "22      4   46.465050        52.105787         5.720444         74.20   \n",
       "23      5   46.465050        51.923396         5.513182         74.10   \n",
       "\n",
       "    CO2 (eur/t)  Endex (eur/Mwh)   AVG_TEMP  current TTF_forward  \\\n",
       "0         25.13            43.19  14.939560               14.566   \n",
       "1         25.14            42.58  18.146067               14.688   \n",
       "2         25.17            48.50  17.863636               14.938   \n",
       "3         25.20            53.24  16.028846               15.713   \n",
       "4         25.30            55.11  12.274436               17.795   \n",
       "5         25.35            55.90   8.689189               20.490   \n",
       "6         25.41            56.20   6.783654               21.035   \n",
       "7         25.47            56.90   4.069565               21.130   \n",
       "8         25.60            55.33   4.152381               21.180   \n",
       "9         25.83            49.75   5.921296               20.759   \n",
       "10        26.29            46.94   8.418750               19.268   \n",
       "11        26.82            45.45  12.776087               18.358   \n",
       "12        27.35            45.63  14.939560               17.853   \n",
       "13        27.88            46.52  18.146067               17.828   \n",
       "14        27.88            49.88  17.863636               18.033   \n",
       "15        27.88            52.54  16.028846               18.738   \n",
       "16        27.88            53.58  12.274436               19.553   \n",
       "17        27.88            54.36   8.689189               20.158   \n",
       "18        27.88            53.21   6.783654               20.618   \n",
       "19        27.88            51.63   4.069565               21.138   \n",
       "20        27.88            49.41   4.152381               20.948   \n",
       "21        27.88            45.77   5.921296               19.968   \n",
       "22        27.88            45.30   8.418750               19.038   \n",
       "23        27.88            45.01  12.776087               18.193   \n",
       "\n",
       "    predicted TTF  market premium  \n",
       "0       16.749908       -2.183908  \n",
       "1       17.100821       -2.412821  \n",
       "2       17.150129       -2.212129  \n",
       "3       18.106243       -2.393243  \n",
       "4       19.850586       -2.055586  \n",
       "5       22.440113       -1.950113  \n",
       "6       22.264355       -1.229355  \n",
       "7       19.772421        1.357579  \n",
       "8       18.924877        2.255123  \n",
       "9       17.766306        2.992694  \n",
       "10      17.739880        1.528120  \n",
       "11      19.005142       -0.647142  \n",
       "12      20.176157       -2.323157  \n",
       "13      19.939632       -2.111632  \n",
       "14      19.520908       -1.487908  \n",
       "15      19.902899       -1.164899  \n",
       "16      21.136690       -1.583690  \n",
       "17      23.004005       -2.846005  \n",
       "18      22.561266       -1.943266  \n",
       "19      21.507097       -0.369097  \n",
       "20      19.827475        1.120525  \n",
       "21      18.894464        1.073536  \n",
       "22      18.800505        0.237495  \n",
       "23      19.609299       -1.416299  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cases = pd.read_csv('Cases.csv',sep=\";\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "display(cases)\n",
    "net = Net(8,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,num_nodes,1).to(torch.device(device))  \n",
    "net.load_state_dict(torch.load('yearly_model_all.pth'))\n",
    "net.eval()\n",
    "\n",
    "TTF_pred = []\n",
    "market_premium = []\n",
    "\n",
    "\n",
    "for i in range(0,cases.shape[0]):    \n",
    "    columns = ['month','% FULL','Brent (eur/bbl)','JKM (Eur/mmbtu)','Coal (eur/t)','CO2 (eur/t)','Endex (eur/Mwh)','AVG_TEMP']\n",
    "    X_use = pd.DataFrame(columns=columns)    \n",
    "    X_use['month'] = [cases[\"month\"].iloc[i]]\n",
    "    X_use[\"% FULL\"] = [cases[\"% FULL\"].iloc[i]]\n",
    "    X_use['Brent (eur/bbl)'] = [cases['Brent (eur/bbl)'].iloc[i]]\n",
    "    X_use['Coal (eur/t)'] = [cases['Coal (eur/t)'].iloc[i]]\n",
    "    X_use['CO2 (eur/t)'] = [cases['CO2 (eur/t)'].iloc[i]]\n",
    "    X_use['JKM (Eur/mmbtu)'] = [cases['JKM (Eur/mmbtu)'].iloc[i]]\n",
    "    X_use['Endex (eur/Mwh)'] = [cases['Endex (eur/Mwh)'].iloc[i]]\n",
    "    X_use['AVG_TEMP'] = [cases['AVG_TEMP'].iloc[i]]\n",
    "\n",
    "    X_use['% FULL']=scaler1.transform(X_use['% FULL'].values.reshape(-1, 1))\n",
    "    X_use['Brent (eur/bbl)']=scaler2.transform(X_use['Brent (eur/bbl)'].values.reshape(-1, 1))\n",
    "    X_use['Coal (eur/t)']=scaler3.transform(X_use['Coal (eur/t)'].values.reshape(-1, 1))\n",
    "    X_use['CO2 (eur/t)']=scaler4.transform(X_use['CO2 (eur/t)'].values.reshape(-1, 1))\n",
    "    X_use['month'] = scaler5.transform(X_use['month'].values.reshape(-1, 1))\n",
    "    X_use['JKM (Eur/mmbtu)'] = scaler6.transform(X_use['JKM (Eur/mmbtu)'].values.reshape(-1, 1))\n",
    "    X_use['Endex (eur/Mwh)'] = scaler7.transform(X_use['Endex (eur/Mwh)'].values.reshape(-1, 1))\n",
    "    X_use['AVG_TEMP'] = scaler8.transform(X_use['AVG_TEMP'].values.reshape(-1, 1))\n",
    "\n",
    "    X_use= torch.Tensor(X_use.astype(np.float32).values)\n",
    "    X_use = X_use.float().to(device) \n",
    "    out = net(X_use).data\n",
    "    out = out.cpu().data.numpy()\n",
    "    TTF_pred.append(out[0][0])\n",
    "    market_premium.append(cases[\"current TTF_forward\"].iloc[i] - out[0][0])\n",
    "    #print(\"predicted price: \",out[0][0])\n",
    "    \n",
    "cases[\"predicted TTF\"] = TTF_pred\n",
    "cases[\"market premium\"] = market_premium\n",
    "display(cases)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(cases[\"predicted TTF\"])), cases[\"predicted TTF\"],label='predicted TTF')\n",
    "plt.plot(np.arange(len(cases[\"current TTF_forward\"])), cases[\"current TTF_forward\"],label='current TTF_forward')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
